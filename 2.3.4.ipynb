{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Iterate and evaluate your classifier\n",
    "In this sheet, I am going to look into the performance of a classifier I made in a previous task. Then I am going to edit it and see if I can make it better. Once I am done, I will answer the questions:\n",
    "\n",
    "Do any of your classifiers seem to overfit?\n",
    "\n",
    "Which seem to perform the best? Why?\n",
    "\n",
    "What features seemed to be most impactful to performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from matplotlib.mlab import PCA as mlabPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jmfra\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Here I am going to load the reviews and their score as well as two other lists. One list contains roughly 2500 buzz words that\n",
    "#we define as \"good\" and the other contains roughly 3300 buzz words we define as \"bad.\" If I were to go through the reviews and\n",
    "#manually select buzz words, that would take way too long, so this is a good place to start.\n",
    "da = pd.read_csv(r\"C:\\Users\\jmfra\\OneDrive\\Documents\\Thinkful Data Science Files\\sentiment labelled sentences\\imdb_labelled.txt\", delimiter= '\\t|\\t1\\n', header=None)\n",
    "da.columns = ['text', 'score']\n",
    "good = pd.read_csv(r\"C:\\Users\\jmfra\\OneDrive\\Documents\\Thinkful Data Science Files\\2.2.7\\positive.txt\", header=None)\n",
    "good.columns = ['x']\n",
    "bad = pd.read_csv(r\"C:\\Users\\jmfra\\OneDrive\\Documents\\Thinkful Data Science Files\\2.2.7\\negative.txt\", header=None)\n",
    "bad.columns = ['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The two lists above are in a pandas dataframe. They are easier to use in a list, so I have converted them to one.\n",
    "goodwords = list(good.values.flatten())\n",
    "badwords = list(bad.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some quick data cleaning.\n",
    "da['text'] = da.text.str.lower()\n",
    "da['text'] = da.text.str.strip()\n",
    "da['text'] = da.text.str.replace('\\.', '')\n",
    "da_t = da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#editing the words after some trials to improve performance\n",
    "goodwords.append('best')\n",
    "goodwords.append('rivet')\n",
    "goodwords.append('cool')\n",
    "goodwords.append('10')\n",
    "goodwords.append('promote')\n",
    "goodwords.append('give this one a look')\n",
    "goodwords.append('pretty decent')\n",
    "goodwords.append('nostalgia')\n",
    "goodwords.append('applause')\n",
    "goodwords.append('liked')\n",
    "goodwords.append('go and see')\n",
    "goodwords.append('must see')\n",
    "goodwords.append('fascinated')\n",
    "goodwords.append('amaze')\n",
    "goodwords.append('pearls')\n",
    "goodwords.append('cult')\n",
    "goodwords.append('taped')\n",
    "goodwords.append('powerful')\n",
    "goodwords.append('cutting edge')\n",
    "goodwords.append('to learn more')\n",
    "goodwords.append('unique')\n",
    "goodwords.append('just right')\n",
    "goodwords.append('really likes')\n",
    "goodwords.append('not bad')\n",
    "goodwords.append('impressed')\n",
    "goodwords = [x for x in goodwords if x != 'improved']\n",
    "goodwords = [x for x in goodwords if x != 'free']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "badwords.append('absolutely no')\n",
    "badwords.append('dislike')\n",
    "badwords.append('disliked')\n",
    "badwords.append('pillow')\n",
    "badwords.append('crap')\n",
    "badwords.append('shattered')\n",
    "badwords.append('not a pleasant')\n",
    "badwords.append('too many')\n",
    "badwords.append('uninteresting')\n",
    "badwords.append('unremarkable')\n",
    "badwords.append('abstruse')\n",
    "badwords.append('kill')\n",
    "badwords.append('sucked')\n",
    "badwords.append('witticisms')\n",
    "badwords.append('1/10')\n",
    "badwords.append('0/10')\n",
    "badwords.append('embarrassed')\n",
    "badwords.append('barely comprehensible')\n",
    "badwords.append('remotely')\n",
    "badwords = [x for x in badwords if x != 'less']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score is the column with the actual decision of if a review is good or bad with a 0 being bad and 1 being good. Let's make this\n",
    "#into a boolean with True and False values to make it easier to use later.\n",
    "da_t['scorenull'] = (da_t['score'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since there are thousands of entries in the good words list, it is easiest to write a for loop that checks to see if each element\n",
    "#has a match in each review. This loop then adds a column to the original dataset with a value of False if they match. This is \n",
    "#because in the previous cell, we set a score of 1(good) to False and we want these outputs to match\n",
    "keywords_good = goodwords\n",
    "\n",
    "for key in keywords_good:\n",
    "    da_t[str(key)] = da_t.text.str.contains(str(key), case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same concept as above but set the output to True\n",
    "keywords_bad = badwords\n",
    "\n",
    "for key in keywords_bad:\n",
    "    da_t[str(key)] = da_t.text.str.contains(str(key), case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing all of the oututs above in a new dataframe, and the real answer in its own as well.\n",
    "data = da_t[keywords_good + keywords_bad]\n",
    "target = da_t['scorenull']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 900 or a 0.9 success ratio\n",
      "Correct False 433 Incorrect False 67\n",
      "Incorrect True 33 Correct True 467\n",
      "Sensitivity 0.934\n",
      "Specificity 0.866\n"
     ]
    }
   ],
   "source": [
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "#Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "#Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "#Display our results.\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target == y_pred).sum(),\n",
    "    ((target == y_pred).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat = confusion_matrix(target, y_pred)\n",
    "print('Correct False {}'.format(conf_mat[0,0]), 'Incorrect False {}'.format(conf_mat[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat[1,0]), 'Correct True {}'.format(conf_mat[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 789 or a 0.789 success ratio\n",
      "Correct False 329 Incorrect False 171\n",
      "Incorrect True 40 Correct True 460\n",
      "Sensitivity 0.92\n",
      "Specificity 0.658\n"
     ]
    }
   ],
   "source": [
    "#After looking through the bernoulli arguments, the only one that will change\n",
    "#the guesses is alpha which is a smoothing parameter automatically set at 1\n",
    "bnb = BernoulliNB(alpha=10)\n",
    "\n",
    "bnb.fit(data, target)\n",
    "\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target == y_pred).sum(),\n",
    "    ((target == y_pred).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat = confusion_matrix(target, y_pred)\n",
    "print('Correct False {}'.format(conf_mat[0,0]), 'Incorrect False {}'.format(conf_mat[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat[1,0]), 'Correct True {}'.format(conf_mat[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 927 or a 0.927 success ratio\n",
      "Correct False 453 Incorrect False 47\n",
      "Incorrect True 26 Correct True 474\n",
      "Sensitivity 0.948\n",
      "Specificity 0.906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jmfra\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "#since it looks as if increasing alpha lowers our accuracy, lets drop it down\n",
    "#to it's lowest possible value\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "\n",
    "bnb.fit(data, target)\n",
    "\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target == y_pred).sum(),\n",
    "    ((target == y_pred).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat = confusion_matrix(target, y_pred)\n",
    "print('Correct False {}'.format(conf_mat[0,0]), 'Incorrect False {}'.format(conf_mat[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat[1,0]), 'Correct True {}'.format(conf_mat[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no matter the alpha, our specificity is always lower than sensitivity, which\n",
    "#in this test, is our ability to properly label a false, or positive review.\n",
    "#this could be because entries with no matching words to our lists are \n",
    "#labaled as false based on the way our code was written. Let's try switching \n",
    "#this to see if it improves our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 927 or a 0.927 success ratio\n",
      "Correct False 474 Incorrect False 26\n",
      "Incorrect True 47 Correct True 453\n",
      "Sensitivity 0.906\n",
      "Specificity 0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jmfra\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "da_n = da\n",
    "da_n['scorenull'] = (da_n['score'] == 1)\n",
    "keywords_good = goodwords\n",
    "\n",
    "for key in keywords_good:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=True)\n",
    "    \n",
    "keywords_bad = badwords\n",
    "\n",
    "for key in keywords_bad:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=False)\n",
    "    \n",
    "data = da_n[keywords_good + keywords_bad]\n",
    "target2 = da_n['scorenull']\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "\n",
    "bnb.fit(data, target2)\n",
    "\n",
    "y_pred2 = bnb.predict(data)\n",
    "\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target2 == y_pred2).sum(),\n",
    "    ((target2 == y_pred2).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat2 = confusion_matrix(target, y_pred2)\n",
    "print('Correct False {}'.format(conf_mat2[0,0]), 'Incorrect False {}'.format(conf_mat2[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat2[1,0]), 'Correct True {}'.format(conf_mat2[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat2[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat2[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the numbers evenly flipped, so it is likely there are no unaccounted for\n",
    "#inputs being labeled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this model, I used a database of good and bad words and then added words I\n",
    "#specifically know improved the success ratio through trial and error. The \n",
    "#only way to create new features would be continuing to add words, or break\n",
    "#down the original thousand plus word lists to see if any outperform the \n",
    "#others or if some can be removed all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2253\n",
      "450.6\n",
      "3923\n",
      "784.6\n"
     ]
    }
   ],
   "source": [
    "#lets make 5 equal groups out of each list\n",
    "print(len(goodwords))\n",
    "print(len(goodwords)/5)\n",
    "print(len(badwords))\n",
    "print(len(badwords)/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodwords1 = goodwords[0:450]\n",
    "goodwords2 = goodwords[450:900]\n",
    "goodwords3 = goodwords[900:1350]\n",
    "goodwords4 = goodwords[1350:1800]\n",
    "goodwords5 = goodwords[1800:2253]\n",
    "badwords1 = badwords[0:784]\n",
    "badwords2 = badwords[784:1568]\n",
    "badwords3 = badwords[1568:2352]\n",
    "badwords4 = badwords[2352:3136]\n",
    "badwords5 = badwords[3136:3923]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 631 or a 0.631 success ratio\n",
      "Correct False 483 Incorrect False 17\n",
      "Incorrect True 352 Correct True 148\n",
      "Sensitivity 0.296\n",
      "Specificity 0.966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jmfra\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "#then run them all through the test to see how well they do.\n",
    "keywords_good = goodwords1\n",
    "\n",
    "for key in keywords_good:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=True)\n",
    "    \n",
    "keywords_bad = badwords1\n",
    "\n",
    "for key in keywords_bad:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=False)\n",
    "    \n",
    "data = da_n[keywords_good + keywords_bad]\n",
    "target2 = da_n['scorenull']\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "\n",
    "bnb.fit(data, target2)\n",
    "\n",
    "y_pred2 = bnb.predict(data)\n",
    "\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target2 == y_pred2).sum(),\n",
    "    ((target2 == y_pred2).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat2 = confusion_matrix(target, y_pred2)\n",
    "print('Correct False {}'.format(conf_mat2[0,0]), 'Incorrect False {}'.format(conf_mat2[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat2[1,0]), 'Correct True {}'.format(conf_mat2[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat2[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat2[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 660 or a 0.66 success ratio\n",
      "Correct False 452 Incorrect False 48\n",
      "Incorrect True 292 Correct True 208\n",
      "Sensitivity 0.416\n",
      "Specificity 0.904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jmfra\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "keywords_good = goodwords2\n",
    "\n",
    "for key in keywords_good:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=True)\n",
    "    \n",
    "keywords_bad = badwords2\n",
    "\n",
    "for key in keywords_bad:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=False)\n",
    "    \n",
    "data = da_n[keywords_good + keywords_bad]\n",
    "target2 = da_n['scorenull']\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "\n",
    "bnb.fit(data, target2)\n",
    "\n",
    "y_pred2 = bnb.predict(data)\n",
    "\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target2 == y_pred2).sum(),\n",
    "    ((target2 == y_pred2).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat2 = confusion_matrix(target, y_pred2)\n",
    "print('Correct False {}'.format(conf_mat2[0,0]), 'Incorrect False {}'.format(conf_mat2[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat2[1,0]), 'Correct True {}'.format(conf_mat2[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat2[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat2[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 662 or a 0.662 success ratio\n",
      "Correct False 449 Incorrect False 51\n",
      "Incorrect True 287 Correct True 213\n",
      "Sensitivity 0.426\n",
      "Specificity 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jmfra\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "keywords_good = goodwords3\n",
    "\n",
    "for key in keywords_good:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=True)\n",
    "    \n",
    "keywords_bad = badwords3\n",
    "\n",
    "for key in keywords_bad:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=False)\n",
    "    \n",
    "data = da_n[keywords_good + keywords_bad]\n",
    "target2 = da_n['scorenull']\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "\n",
    "bnb.fit(data, target2)\n",
    "\n",
    "y_pred2 = bnb.predict(data)\n",
    "\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target2 == y_pred2).sum(),\n",
    "    ((target2 == y_pred2).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat2 = confusion_matrix(target, y_pred2)\n",
    "print('Correct False {}'.format(conf_mat2[0,0]), 'Incorrect False {}'.format(conf_mat2[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat2[1,0]), 'Correct True {}'.format(conf_mat2[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat2[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat2[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 615 or a 0.615 success ratio\n",
      "Correct False 143 Incorrect False 357\n",
      "Incorrect True 28 Correct True 472\n",
      "Sensitivity 0.944\n",
      "Specificity 0.286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jmfra\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "keywords_good = goodwords4\n",
    "\n",
    "for key in keywords_good:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=True)\n",
    "    \n",
    "keywords_bad = badwords4\n",
    "\n",
    "for key in keywords_bad:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=False)\n",
    "    \n",
    "data = da_n[keywords_good + keywords_bad]\n",
    "target2 = da_n['scorenull']\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "\n",
    "bnb.fit(data, target2)\n",
    "\n",
    "y_pred2 = bnb.predict(data)\n",
    "\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target2 == y_pred2).sum(),\n",
    "    ((target2 == y_pred2).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat2 = confusion_matrix(target, y_pred2)\n",
    "print('Correct False {}'.format(conf_mat2[0,0]), 'Incorrect False {}'.format(conf_mat2[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat2[1,0]), 'Correct True {}'.format(conf_mat2[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat2[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat2[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 713 or a 0.713 success ratio\n",
      "Correct False 458 Incorrect False 42\n",
      "Incorrect True 245 Correct True 255\n",
      "Sensitivity 0.51\n",
      "Specificity 0.916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jmfra\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "keywords_good = goodwords5\n",
    "\n",
    "for key in keywords_good:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=True)\n",
    "    \n",
    "keywords_bad = badwords5\n",
    "\n",
    "for key in keywords_bad:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=False)\n",
    "    \n",
    "data = da_n[keywords_good + keywords_bad]\n",
    "target2 = da_n['scorenull']\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "\n",
    "bnb.fit(data, target2)\n",
    "\n",
    "y_pred2 = bnb.predict(data)\n",
    "\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target2 == y_pred2).sum(),\n",
    "    ((target2 == y_pred2).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat2 = confusion_matrix(target, y_pred2)\n",
    "print('Correct False {}'.format(conf_mat2[0,0]), 'Incorrect False {}'.format(conf_mat2[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat2[1,0]), 'Correct True {}'.format(conf_mat2[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat2[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat2[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly labeled points out of a total 1000 points : 900 or a 0.9 success ratio\n",
      "Correct False 464 Incorrect False 36\n",
      "Incorrect True 64 Correct True 436\n",
      "Sensitivity 0.872\n",
      "Specificity 0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jmfra\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "keywords_good = goodwords1 + goodwords2 + goodwords3 + goodwords5\n",
    "\n",
    "for key in keywords_good:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=True)\n",
    "    \n",
    "keywords_bad = badwords1 + badwords2 + badwords3 + badwords5\n",
    "\n",
    "for key in keywords_bad:\n",
    "    da_n[str(key)] = da_n.text.str.contains(str(key), case=False)\n",
    "    \n",
    "data = da_n[keywords_good + keywords_bad]\n",
    "target2 = da_n['scorenull']\n",
    "bnb = BernoulliNB(alpha=0)\n",
    "\n",
    "bnb.fit(data, target2)\n",
    "\n",
    "y_pred2 = bnb.predict(data)\n",
    "\n",
    "print(\"Number of correctly labeled points out of a total {} points : {} or a {} success ratio\".format(\n",
    "    data.shape[0],\n",
    "    (target2 == y_pred2).sum(),\n",
    "    ((target2 == y_pred2).sum())/data.shape[0]\n",
    "))\n",
    "conf_mat2 = confusion_matrix(target, y_pred2)\n",
    "print('Correct False {}'.format(conf_mat2[0,0]), 'Incorrect False {}'.format(conf_mat2[0,1]))\n",
    "print('Incorrect True {}'.format(conf_mat2[1,0]), 'Correct True {}'.format(conf_mat2[1,1]))\n",
    "print('Sensitivity {}'.format(conf_mat2[1,1]/500))\n",
    "print('Specificity {}'.format(conf_mat2[0,0]/500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each group seems to equally identify False values properly, but the 4th group\n",
    "#correctly identifies True values at a rediculous rate in comparison to the \n",
    "#rest. If I was to try and perfect this model, looking at this group would\n",
    "#be the most beneficial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do any of your classifiers seem to overfit?\n",
    "\n",
    "With the multitudes of words, it is unlikely the original sets of good and bad words overfit to my data. This word list was not created with the thought of this specific list. My added words are very likeely to over fit to this data, however, because they were added to the list with specific entries in mind. I also only had this input list to test if each addition added to accuracy, so they are unlikely to be as efficient on different data sets of the same type.\n",
    "\n",
    "Which seem to perform the best? Why?\n",
    "\n",
    "The words with the \"best\" performance are the most general words. For example, \"cool\" was used many more times than \"not too bad\" and added more correct responses when added to the set. This is a double edged sword however, becasue there was no context used in this classifier and the word cool is also much more likely to be in a bad review where they were being sarcastic or mentioned something ok before talking about the negatives then the phrase \"not too bad.\" This is true because in order to increase success ratio, you need to identify the most number of responses in the least number of inputs. The broad words are more likely to produce false responses, but they affect a much larger number in general.\n",
    "\n",
    "As for the broken down lists of words, the first group correctly identifies the most Falses, or positive reviews and the fourth group overwhelmingly identifies the most Trues, or negative reviews. It is likely that by disecting these two groups and selecting specific words out of them through trial and error, we could attain the best success ratio.\n",
    "\n",
    "What features seemed to be most impactful to performance?\n",
    "\n",
    "The 4th group of bad words had the most impactful performance because the other 4 groups did a pretty good job of correctly identifying positive reviews (all around 90% success), but this one beat the others in correctly identifying the negative reviews by a wide margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
